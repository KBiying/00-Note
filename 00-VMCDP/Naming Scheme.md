### Naming Scheme

The process of ICN multicast, including infinite state machine and explanation
(need mention inter-process communication)

ç®€å•ä»‹ç»ERLç³»ç»Ÿå¤§æ¦‚æ˜¯è¦åšä»€ä¹ˆï¼šé¦–å…ˆERLè¿›è¡Œæœ¬åœ°è®­ç»ƒï¼Œå¾—å‡ºæ€§èƒ½æŒ‡æ ‡ã€‚agentsç›¸äº’è¯·æ±‚æ€§èƒ½æŒ‡æ ‡æ•°æ®ã€‚æ¯ä¸ªagentæ ¹æ®å…¨å±€æ€§èƒ½æŒ‡æ ‡æ¥è¯„åˆ¤è‡ªå·±çš„å­¦ä¹ æ•ˆæœï¼Œä»¥åŠè¯„ä¼°å‡ºæ¯”è‡ªå·±å­¦ä¹ æ•ˆæœå¥½çš„agentï¼Œå¹¶å‘ä¹‹è¯·æ±‚æ•°æ®ç”¨äºè‡ªèº«è®­ç»ƒã€‚



ä¸ºæ­¤ï¼Œåœ¨ä¼ è¾“å±‚ä¸­åˆ†ä¸¤ä¸ªé˜¶æ®µæ»¡è¶³ERLçš„æ•°æ®ä¼ è¾“éœ€æ±‚ã€‚

ç¬¬ä¸€é˜¶æ®µï¼šagentsç›¸äº’è¯·æ±‚æ€§èƒ½æŒ‡æ ‡æ•°æ®ï¼Œå› æ­¤ä¼ è¾“å±‚éœ€è¦å¹¿æ’­è¯·æ±‚neigbourçš„æ€§èƒ½æŒ‡æ ‡

ç¬¬äºŒé˜¶æ®µï¼šè¯·æ±‚æ•°æ®ç”¨äºè‡ªèº«è®­ç»ƒ

æ ¹æ®è¿™ä¸¤ä¸ªé˜¶æ®µçš„éœ€æ±‚ï¼Œæˆ‘ä»¬è®¾è®¡äº†naming scheme

ç¬¬ä¸€é˜¶æ®µmulticastï¼š/broadcast/performanceIndexã€‚å„ä¸ªagentå‘ç½‘ç»œå‘é€è¯¥intereståŒ…ï¼Œå…¶ä»–agentsæ”¶åˆ°intereståŒ…åè¿”å›è‡ªå·±çš„performance indexã€‚æ”¶åˆ°æ¥è‡ªå„agentçš„performanceindexã€‚å½“ç„¶å•¦ï¼Œç”±äºç½‘ç»œä¸ç¨³å®šæ€§ï¼Œæœ‰å¯èƒ½æ”¶ä¸é½ã€‚æ­¤æ—¶å°±éœ€è¦é‡ä¼ ã€‚

ç¬¬äºŒé˜¶æ®µæ ¹æ®subsetè¯·æ±‚ã€‚/parameters/<>/<>/<identifier> identifierå…·æœ‰å”¯ä¸€æ€§ï¼Œæ ‡è¯†æ¯æ¬¡è¿­ä»£è®­ç»ƒçš„ç¼–å·ï¼Œã€‚è¯·æ±‚å›æ¥çš„æ•°æ®æ˜¯ä»€ä¹ˆ

ä¸¤ä¸ªé˜¶æ®µè¦è¯·æ±‚çš„æ•°æ®å†…å®¹å­˜æ”¾åˆ°payloadå½“ä¸­ï¼Œç®€è¦ä»‹ç»éœ€è¦å…±äº«çš„æ•°æ®å†…å®¹ï¼šperformance index which is a float type å’Œparameters which is a list contains multi float type number. 

æˆ‘ä»¬é€šè¿‡è¿›ç¨‹é—´å®ç°çŠ¶æ€çš„è½¬åŒ–ã€‚

æè¿°æœ‰é™çŠ¶æ€æœºï¼šWe implement state transitions through inter-process. The finite-state machine (FSM) definitions for the ERL and ICN are shown in Fig. 1. The FSM in figure 1.a defines the operation of the ERL, while the FSM in figure 1.b defines the operation of the ICN. The arrows in the FSM description represent the protocol's movement from one state to another. The dashed arrow represents the FSM's initial state, assumed to be S0. Besides, these transitions represent cyclical behavior, where the machine loops back to S0 after reaching the final state.

The ERL FSMs in Fig. 1 have 6 states, which will be denoted as S0, S1, S2, S3, S4, and S5. Each state is distinct and identifiable. At the beginning, the ERL application is waiting for the start signal issued by ICN. It enters state S1 after receiving the start signal to train a neural network. Following training, ERL generates a performance index that indicates the training performance of this iteration and then sends it to ICN. After transmission, it enters S2 and waits for ICN to collect performance indices accompanied by an identifier from surrounding agents. Compare the collected performance indices in state S3. If the performance of local is so good that it doesn't need to learn from others, ERL will send an empty subset and optimized parameters of local to ICN, then finally return to state S0. Otherwise, ERL will send a subset containing identifiers indicating the learning object and optimized local parameters to ICN, then wait for collected parameters to be collected through ICN in state S4. Finally, ERL gathers the collected parameters from ICN and merges the paras for better and more accurate predictions or decisions. Then roll back to state S0, waiting for a new round of training.

Likewise, the ICN FSMs in Fig. 1(b) have 5 states. Initially in state S0, the FSM awaits its own performance index from the Emssemble Reinfoce Learning(ERL). It moves to state S1 after obtaining the performance index, where it gathers performance indices from neighboring agents. After returning the collecting results to ERL, in state S2, the ICN awaits for its own parameters and subset . If ERL returns an empty subset which means this agent learns very well, so it does not need to learn from others and then returns to S0 to await the next ERL performance index. If ERL return a non-empty subset, it advances to state S3 and begins collecting parameters from neighboring agents. After gathering the required parameters, the ICN advances to state S4 to begin the next round by delivering the collected parameters. Finally, roll back to state S0 by sending start signal.



Similarly, the ICN FSMs in Fig. 1(b) contain five states. The FSM is initially in state S0, awaiting its own performance index from the Emssemble Reinfoce Learning (ERL). After acquiring the performance index, it proceeds to state S1, where we collect performance indices from nearby agents through ICN.In stage S2, the ICN awaits its own parameters and subset after submitting the collecting results to ERL. If ERL returns an empty subset, this agent has learned very well and does not need to learn from others, and it returns to S0 to await the next ERL performance index. If ERL returns a non-empty subset, the process moves to S3 and continues collecting parameters from surrounding agents. The ICN advances to state S4 to start the following round by sending the appropriate parameters gathered. Finally, transmit the start signal to return to state S0.



å¼€å§‹å‘½ä»¤æ˜¯ICNå‘å‡ºçš„ï¼Œæ ‡å¿—ç€ICNå·²ç»å‡†å¤‡å¥½æ¥æ”¶ERLçš„æ•°æ®äº†ï¼šThe initiation command, which is issued by Information-Centric Networking (ICN), serves as a pivotal signal indicating that ICN is now prepared to receive data from the ERL (External Resource Location). 

ä¹¦æœ¬å¯¹çŠ¶æ€æœºçš„æè¿°ï¼šThe finite-state machine (FSIM) definitions for the rdt1. O sender and receiver are shown in Figure 3.9. The FSM in Figure 3.9(a) defines the operation of the sender, while the FSM in Figure 3.9(b) defines the operation of the receiver. It is important to note that there are separate FSMs for the sender and for the receiver. The sender and receiver FSMs in Figure 3.9 each have just one state. The arrows in the FSM description indicate the transition of the protocol from one state to another. (Since each FSM in Figure 3.9 has just one state, a transition is necessarily from the one state back to itself; we'll see more complicated state diagrams shortly.) The event causing the transition is shown above the horizontal line labeling the transition, and the actions taken when the event occurs are shown below the horizontal line. When no action is taken on an event, or no event occurs and an action is taken, we'll use the symbol A below or above the horizontal, respectively, to explicitly denote the lack ot an action or event. The initial state of the FSM is indicated by the dashed arrow. Although the FSMs in Figure 3.9 have but one state, the FSMs we will see shortly have multiple states, so it will be important to identify the initial state of each FSM.

ä»Šå¤©çš„ä»»åŠ¡æ˜¯æŠŠæµç¨‹å’Œnamingç†é¡ºï¼Œå†™å‡ºè®ºæ–‡æè¿°

å®éªŒè®¾è®¡

jetsonè·‘piåšä¼ è¾“çš„äº‹æƒ…ä¸€å¥è¯å¸¦è¿‡ï¼Œä¸»è¦å¼ºè°ƒçš„æ˜¯æˆ‘ä»¬å¼€çš„æ˜¯è¿›ç¨‹å¤„ç†è¿™æ ·çš„äº‹æƒ…ã€‚

Thurther workï¼š

1.cs

### FLARE: Federated Active Learning Assisted by Naming for Responding to Emergencies

#### 3.1 Name-based Pub/Sub in FLARE

The communication between entities in FLARE is name-based (to enable information centricity [33]), used both for disseminating instructions and reports between first responders, and also learning/inference-related information sharing among SMEs. 

ä»–ä»¬ä¹‹é—´çš„communicationé€šè¿‡name-basedï¼Œ ç”¨è¿™ä¸ªæ¥disseminating instructions and reports.

A unified namespace, such as the one in Fig. 2, guides this informationcentric framework, and is a common interface that various primary actors and components (i.e., first responders, SMEs, and Incident Coordinator but not all the other social media users) have a local copy of, and **can use to indicate which subset of the incident namespace they are interested in (i.e., subscribed to), and want to deliver to**. First responders, based on their assigned task (either based on the incident or as part of their organizational hierarchy), **subscribe to a prefix in the namespace. For example, a firefighter FF1 assigned with tasks related to â€œFire Engine 1â€ will subscribe to â€œ/IncidentX/LA/Fire/Firefighting/FireEngine1â€.**

 The subscription can be at any desired granularity: e.g., another firefighter, let us call him/her FF2, who manages all fire engines in LA, will subscribe to â€œ/IncidentX/LA/Fire/Firefightingâ€. **Any publication ğ‘ƒ1 with a prefix** â€œ/IncidentX/LA/Fire/Firefightingâ€ **will be delivered to** FF2. Additionally, FLARE expands the delivery according to the name hierarchy to reach all relevant recipients.

ç®€å•ä»‹ç»ä¸€ä¸‹ä¸€äº›subscription

 **To provide that capability in a flexible and efficient manner, here we use an NDN architecture that is enhanced with recipient-based pub/sub logic (as proposed in [8]).** This forwarding logic matches an incoming packet with all entries that contain the packetâ€™s name as a prefix. Thus, ğ‘ƒ1 will be delivered to the subscribers of names with the prefix containing â€œ/IncidentX/LA/Fire/Firefightingâ€, including FF1 as well. In addition to the prefix, the publication names in FLARE can **include other attributes as well,** such as tags and an arbitrary number of input parameters. The named-SMP (NSMPs) containing instructions for first responders **will be published (typically by SMEs) with the following name format:** â€œ/[prefix]/tag=instr/[params]â€. **The prefix** (e.g., â€œ/IncidentX/LA/Fire/Firefightingâ€) will be used to forward the publication to the right recipients. **The tag** is used to indicate the type of the message payload (e.g., instr). **The params** are input parameters used by the recipients to process the received information, as in â€œ.../ğ‘ğ‘ğ‘Ÿğ‘ğ‘š1 = ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’1/ğ‘ğ‘ğ‘Ÿğ‘ğ‘š2 = ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’2/...â€. FLARE also uses name-based delivery for message exchange among SMEs **in its Federated Learning-based procedures**, integrating the namespace with different classifiers. Each SME is organizationally in charge of processing and manually labeling SMPs associated with certain prefixes, which they subscribe to. E.g., ğ‘†ğ‘€ğ¸1 in Fig. 1 subscribes to â€œ/IncidentX/LA/Fire/SMEâ€. The dispatcher with ğ‘†ğ‘€ğ¸1 has specialized domain knowledge to label SMPs regarding â€œFireâ€ and its sub-categories, e.g., â€œSurvivalSearchâ€, etc.

 There are three types of SME-related messages: 1) SME-to-SME (S2S) messaging: An SME asking any SME or a particular SME with the prefix ğ‘ to process an SMP (for calculating, labeling, etc.): â€œ/[ğ‘]/tag=proc/[ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘ ]â€, **where ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘  can include SMP ID, confidence values, etc.** The payload of this message type is SMP content (e.g., a tweet json). 2) SME-to-Incident Coordinator (S2I) messaging: An SME sends its processing result from training to incident coordinator for aggregation: â€œ/IncidentX/Coord/ tag=result/[ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘ ]â€, where ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘  can include which classifier it includes the result it is associated with (whether C1 or C2). The payload of this message type is the calculation result, e.g., weights. 3) I2S messaging: The incident coordinator distributing fully trained models to a set of SMEs under prefix ğ‘ for synchronization: â€œ/[ğ‘]/tag=model/[ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘ ]â€, where ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘  can include the classifier, version number, etc. The payload of this message is the most recent, fully trained model, after the incident coordinatorâ€™s aggregation procedure is completed. The namespace has a hierarchical structure (as a prefix tree). The hierarchical levels are divided into three levels, namely root, static, and dynamic levels to capture both how the namespace nodes are managed, and their relationship with the SME classification procedures. The root identifies the name of the namespace. The static levels follow the organizational/incident command structure, based on a template, created at the beginning, when the response to the incident is initiated and organized. They remain static during the disasterâ€™s management. The dynamic levels represent more fine-grained roles under the static levels. **Name nodes within the dynamic levels can be created, modified or removed as the disaster unfolds, based on command decisions.**

ä»‹ç»æœ‰ä¸‰ç§æƒ…å†µçš„nameing



We have 3 type of interest name.

1) Discover performance index from neighbour agents in phase 1: /pfm/discovery.

2) Request specific parameters from neighbour agents in phase 2: /paras/<identifier>

3) Collect parameters from neighbour agents in phase 3: /paras/discovery



ç¬¬ä¸€é˜¶æ®µçš„å†…å®¹ï¼šæ›´æ–°è‡ªå·±çš„pfmåï¼Œ æ¯æ¬¡è®­ç»ƒéƒ½ä¼šäº§ç”Ÿå”¯ä¸€çš„identifierå’Œpfmä»¥åŠparasã€‚è®­ç»ƒçš„æ€§èƒ½æŒ‡æ ‡pfmåœ¨ä¸€å®šç¨‹åº¦ä¸Šè¡¨ç¤ºè¯¥agentè®­ç»ƒçš„æƒ…å†µæ˜¯ä¼˜è¿˜æ˜¯åŠ£ã€‚å»å‘ç°é™„è¿‘æ™ºèƒ½ä½“çš„pfmï¼Œæ”¶é›†èµ·æ¥ä¹‹åé€šè¿‡åˆ†æpfmï¼Œæ¥ç¡®å®šå“ªä¸ªpfmå€¼å¾—å­¦ä¹ ã€‚è¦å®Œæˆçš„æ•°æ®ä¼ è¾“ç›®æ ‡æ˜¯ï¼šé€šè¿‡pub/subæ¥å‘ç°å’Œæ”¶é›†é™„è¿‘æ™ºèƒ½ä½“çš„pfmï¼Œè¯·æ±‚å…´è¶£åŒ…çš„åç§°ä¸º/pfm/discoveryã€‚å½“agentæ”¶åˆ°è¯¥å…´è¶£åŒ…çš„æ—¶å€™ï¼Œå°±ä¼špublishè‡ªå·±çš„æœ€æ–°è¿­ä»£çš„pfmå’Œæ ‡å¿—è¯¥è¿­ä»£çš„identifierã€‚

The task of first phase: In Emssemble learning, each trainning iteration will generate a sole identifier, perforamance index and parameters. After one round trainning, we update performance index which can indicate the trainning performance to some extend. So each agent get updated performance now, we need to share the performance to figure out which one we can learn from. Therefore, The transmittion task is to discover and collect performance index of neigbor agents through pub/sub protocol. The interest name is designed as /pfm/discovery. While someone receive this interest package, they will publish/response with their latest performance index and corresponding identifier. 

ç¬¬äºŒé˜¶æ®µçš„å†…å®¹ï¼šç¡®å®šå¥½éœ€è¦å­¦ä¹ çš„å¯¹è±¡ä¹‹åï¼Œå‘è¯¥å¯¹è±¡è¯·æ±‚è®­ç»ƒå‚æ•°ï¼Œå¥½çš„å‚æ•°å¯ä»¥æ›´å¿«åœ°æé«˜è‡ªå·±çš„è®­ç»ƒèƒ½åŠ›ã€‚ç”±äºæ¯æ¬¡è®­ç»ƒéƒ½ä¼šäº§ç”Ÿå”¯ä¸€çš„identifierå’Œpfmä»¥åŠparasï¼Œ å› æ­¤å¯é€šè¿‡å”¯ä¸€å€¼ï¼Œå»è·å–identifierç›¸å¯¹åº”çš„é‚£è½®å‚æ•°ã€‚è¦å®Œæˆçš„æ•°æ®ä¼ è¾“ç›®æ ‡æ˜¯ï¼šé€šè¿‡pub/subæ¥å‘ç°å’Œæ”¶é›†paras of well-trained agent. ç”±äºpub/subè§„å®šï¼Œè¯·æ±‚æ•°æ®çš„åç§°è¦ä¸å†…å®¹ç›¸å…³ï¼Œè€Œä¸ä½ç½®æˆ–ç”Ÿäº§è¯¥æ•°æ®çš„å¯¹è±¡æ— å…³ã€‚å› æ­¤æˆ‘ä»¬é€šè¿‡è¯·æ±‚æŸä¸€è½®çš„identifieræ¥è·å–åˆ°è¯¥parasï¼Œè¯·æ±‚çš„å…´è¶£åŒ…åç§°ä¸º/paras/<identifier>ã€‚identifierä¸ºä¸€ä¸ª64bitçš„hashå€¼ï¼Œå¯ä»¥å”¯ä¸€æ ‡è¯†æŸæ¬¡è¿­ä»£è®­ç»ƒã€‚å½“æ‹¥æœ‰è¯¥parasçš„agentæ”¶åˆ°è¯¥å…´è¶£åŒ…åï¼Œä¼šè¿”å›å¯¹åº”çš„parasã€‚

The task of second phase: After collecting the performance index, ERL will choose the best one or two performance index that represents well-trained and is better than self performance. Then we will request the trainning parameters of well-trained agent through corresponding identifier. The interest name is designed as  /paras/<identifier>. Identifier is a 64bit hashed number, which can identify the iteration of trainning. The agent who own this parameter will response the  package.

åœ¨ç¬¬ä¸‰é˜¶æ®µï¼Œç§°ä¸ºæ€»éªŒè¯é˜¶æ®µï¼Œé€‰å–1ä¸ªagentï¼Œ è·å–æ‰€æœ‰agentçš„æœ€åä¸€æ¬¡è¿­ä»£çš„parasï¼Œåšèåˆã€‚ç›®çš„æ˜¯è·å–é›†åˆä¹‹åçš„æ¨¡å‹å‚æ•°ã€‚è¦å®Œæˆçš„æ•°æ®ä¼ è¾“ç›®æ ‡æ˜¯ï¼šé€šè¿‡pub/subæ¥è¯·æ±‚é‚»è¿‘æ‰€æœ‰agentçš„è®­ç»ƒæˆæœã€‚ï¼ˆä¸‡ä¸€æ²¡æœ‰è®­ç»ƒå®Œå‘¢ï¼Ÿï¼‰ä½œå‚æ•°èåˆï¼Œå¯ç»¼åˆå‡ºæœ€åçš„æ¨¡å‹å‚æ•°ã€‚

æ­¤æ–¹æ¡ˆï¼Œä¸ä»…å¯ä»¥é«˜æ•ˆå®Œæˆé€šè¿‡å¤šæ’­çš„æ•°æ®ä¼ è¾“ ä»¥åŠ åº”å¯¹åŠ¨æ€åŠ å…¥çš„æƒ…å†µã€‚

æˆ‘ä»¬è¦å®Œæˆçš„ç›®æ ‡å°±æ˜¯ä¸ºERL share infomation æä¾›æ•°æ®ä¼ è¾“å¹³å°

The communication between agents in XXX is name-based (to enable information centricity[xx]), This method is applied for the dissemination of performance indices as well as the sharing of parameters between agents. In this case, we employ an NDN architecture supplemented with recipient-based pub/sub logic (as presented in [xx]).

The objective of the first phase: In Ensemble Learning, each training iteration generates a unique identifier, performance index, and parameters. After one training round, the performance index is updated, which can indicate the training performance to some extent. As each agent receives updated performance, it is necessary to share the performance to determine which one can be learned from. Thus, the transmission task involves discovering and collecting the performance index of neighbouring agents through the pub/sub logic. The name of the interest is designated as /pfm/discovery. When an individual receives this interest package, they will publish a response that includes their latest performance index and corresponding identifier.

In the second phase of the project: Once the performance indices have been collected, ERL will select one or two performance indices that accurately represent well-trained models and outperform self-performance. Subsequently, the training parameters of the well-trained agent will be obtained through the relevant identifier. The identifier corresponds to the interest name /paras/<identifier>. The identifier is a 64-bit hashed number that is capable of identifying the training iteration. The agent that possesses these parameters will then respond to the package.

